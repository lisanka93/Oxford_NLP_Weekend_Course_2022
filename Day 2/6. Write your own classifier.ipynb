{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a3879e",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We will be using a twitter dataset. (I reduced it a bit to reduce waiting times during training + to have clean numbers 30k training data 3k hold-out set)\n",
    "\n",
    "https://www.kaggle.com/edqian/twitter-climate-change-sentiment-dataset\n",
    "\n",
    "Each tweet is labelled as one of the following classes:\n",
    "\n",
    "    2(News): the tweet links to factual news about climate change\n",
    "    1(Pro): the tweet supports the belief of man-made climate change\n",
    "    0(Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "    -1(Anti): the tweet does not believe in man-made climate change\n",
    "\n",
    "1. Apply the preprocessing function from yesterday on the dataset, however with the following modification: twitter data is very dirty, with many typos and rare words which have no value for the model. For now, do not delete stopwords, but delete all words that only come up 1 or 2 times in the whole corpus. The following functions will be useful:\n",
    "    - `df.message.tolist()` to store all messages in a list\n",
    "    - `from collections import Counter`\n",
    "    - `df.messages.apply(your_function)` to apply a function on a column\n",
    "    \n",
    "### Training\n",
    "    \n",
    "2. Set up a baseline model (Naive Bayes) using the raw data and compare it to the cleaned data.\n",
    "\n",
    "3. Experiment with n_gram range - which n_gram range gives the best results?\n",
    "\n",
    "4. Add stopwords to the Countvectorizer - does that make the results better or worse?\n",
    "\n",
    "5. Change the Countvectorizer for a Tfidf vecrotizer - which one performs better? Read up on Tfidf vectorizer\n",
    "\n",
    "6. Try a different classifier (e.g. Logistic Regression) - which one performs better? And why does it take longer to train that classifier compared to naive bayes? (set max_iter paramter to 1000 (max_iter=1000) to avoid getting a warning and for better performance. It will take a minutes or so to train)\n",
    "\n",
    "7. The category \"news\" does not really fit with the other 3 categories (anti, neutral and pro) - delete the newscategory from the dataset and check how well your model performs. Does it perform better now that news is gone?\n",
    "\n",
    "### Evaluating\n",
    "\n",
    "8. Print out the confusion matrix (for the 3 class classificaion) and interpret it - what does the model struggle with?\n",
    "\n",
    "9. Now create a new column which shows 0 if it is news (2 in sentiment column) and 1 for not news (-1,0,1) in sentiment column and use your model on that column (classifying whethr something is news or not news) - how does the model perform? Does this support the hypothesis that \"news\" doesnt really fit with the other 3 categories\n",
    "\n",
    "10.  Now suppose you use your model to first distinguish news from non news and then use the model on the other 3 categories. Should that approach work better in theory?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe2c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

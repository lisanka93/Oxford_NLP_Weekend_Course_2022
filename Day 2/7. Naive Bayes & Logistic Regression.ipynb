{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 - Naive-Bayes and Logistics Regression in NLP\n",
    "\n",
    "In this notebook, we will make use of the knowledge gained in the previous one and will go over two fundamental classification algorithms in detail. We will cover:\n",
    "- Naive-Bayes classifier fundamentals\n",
    "- NB application in NLP\n",
    "- smoothing\n",
    "\n",
    "and\n",
    "\n",
    "- Logistic Regression fundamentals\n",
    "- LR loss function & regularization\n",
    "- LR learning process \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Naive-Bayes Classifier\n",
    "\n",
    "As stated in the previous notebook, Naive-Bayes is a supervised learning probabilistic classifier. It is based on applying Bayes' probability theorem and using the fact that the occurrence of an event impacts the probability of another event. But how exactly does it work?\n",
    "\n",
    "### 8.1 NB Fundamentals\n",
    "The general purpose of classifiers is to *classify* samples from the dataset into 2 or more **classes**. Since we want to classify text, instead of the term *sample* we will use the term **document**. Thus, classifiers' task is to take an input document *d* and out of all possible classes, return a class *c*, to which the document *d* belongs.\n",
    "\n",
    "Now, since NB is the probabilistic classifier, its role would be to **maximize the probability** of the predicted class c given the input document d.\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq1.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "The intuition of Bayesian classification is to use **Bayes’ rule** to transform the equation above into their probabilities that have some useful properties.\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq2.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "We then substitute the first equation into the second one to get:\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq3.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "We can conveniently simplify the above equation by dropping the denominator *P(d)*. This is possible because we will be computing *P(d|c)P(c) / P(d)* for each possible class, but *P(d)* does not change for each class; we are always asking the most likely class for the same article *d*, which must have the same probability *P(d)*. Thus, we can choose the class that maximizes the simpler formula\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq4.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "Okay, but how do we represent a document *d*? We can represent a document as a set of **features** `d = (f1, f2, f3 ... fn)`. One way to define these features is to use the Bag-of-words model introduced in Notebook 2. After constructing the BOW of the complete dataset, we will be able to express each document as a vector of word counts. Thus, we can treat each vector value associated with a different word as a separate feature giving us information on the words (and optionally their counts) used in the document. Here we also introduce the first of two **simplifying assumptions**: since we use BOW, the **word order doesn't matter**. We don't care about the position of a word in a document.\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq5.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "However, calculating `P(f1, f2, f3 ... fn | c)` requires computing all possible combinations of features (if BOW uses sum pooling than even more!). We need another simplifying assumption called the **naive Bayes assumption** - the **conditional independence between features** given the same class. Hence, we can multiply probabilites as follows:\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq6.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "Resulting in a final equation:\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq8.png\" alt=\"equation\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 NB Training\n",
    "\n",
    "So how do we train the classifier? How does it learn what is *P(c)* and *P(f|c)*? Starting with the first probability, we can simply use frequencies and derive it from the probability definition: the probability of a class in the dataset is the number of documents of this class divided by the total number of all documents. \n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq7.png\" alt=\"equation\" width=\"200\"/></div>\n",
    "\n",
    "Learning the probability of features given a class *P(f<sub>i</sub>|c)* isn't more complicated. We assume a feature is just the existence of a word in the document’s bag of words (set of the vocabulary *V*), and so we’ll want *P(w|c)*, which we compute as **fraction of times the word w<sub>i</sub> appears among all words in all documents of topic c**.\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq9.png\" alt=\"equation\" width=\"400\"/></div>\n",
    "\n",
    "Let's consider the following example:\n",
    "\n",
    "This is our training data:\n",
    "\n",
    "| **Text** | **Labels** |\n",
    "|----------|-----------|\n",
    "|\"What a great match\" | sports |\n",
    "|\"The election results will be out tomorrow\"| not sports |\n",
    "|\"The match was very boring\"| sports |\n",
    "|\"It was a close election\"| not sports |\n",
    "\n",
    "To make the example easier to follow, let’s assume we applied some pre-processing to the sentences and removed stopwords. The resulting sentences are:\n",
    "\n",
    "| **Text** | **Labels** |\n",
    "|----------|------------|\n",
    "|\"great match\"|  sports |\n",
    "|\"election results tomorrow\"| not sports |\n",
    "|\"match boring\"| sports |\n",
    "|\"close election\"| not sports |\n",
    "\n",
    "In our small corpus, we have 2 classes each having 2 senteces. Hence, the probability of each class is:\n",
    "\n",
    "        P(\"sports\") = 2/4 = 0.5\n",
    "        P(\"not sports\") = 2/4 = 0.5\n",
    "\n",
    "Total unique features (words) for \"sports\": 3\n",
    "Total unique features (words) for \"not sports\": 4\n",
    "\n",
    "Let's say we want to assign a class to the following sentence \"that was a very close, great match\". After stop word removal it is \"**close great match**\". Now we need to perform some calculations:\n",
    "\n",
    "1. Likelihood P(\"close great match\"|sports) = P(\"close\"|sports) * P(\"great\"|sports) * P(\"match\"|sports)\n",
    "2. Likelihood P(\"close great match\"|not sports) = P(\"close\"|not sports) * P(\"great\"|not sports) * P(\"match\"|not sports)\n",
    "\n",
    "| word | P(word\\|sports) | P(word\\|not sports)|\n",
    "|------|----------------|-------------------|\n",
    "| close | 0/3 | 1/4 |\n",
    "| great | 1/3| 0/4|\n",
    "| match | 2/3| 0/4|\n",
    "\n",
    "We suspect that the correct class is \"sport\", right? Let's see what happens with the likelihood:\n",
    "P(\"close great match\"|sports) = 0/3 * 1/3 * 2/3\n",
    "P(\"close great match\"|sports) = 0\n",
    "\n",
    "Oops... This example shows a very common situation - there were no training documents classified as \"sports\" containing the word \"close\". As a result, P(\"close\"|\"sports\") results in a painful zero, which also makes the product of probabilities equals 0. We can solve this issue using **smoothing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 NB Smoothing & Unknown words\n",
    "\n",
    "\n",
    "#### Laplace smoothing\n",
    "\n",
    "Smoothing is used to avoid a situation that the classifier assigns zero probability to the whole document (as we can see above). This happens when a classifier sees a word, which IS present in the vocabulary (perhaps in a document of a different class), but it wasn't used in the given context. The intuition behind the smoothing is that we don't want the classifier to assign zero probabilities to previously unseen events - the fact that something wasn't present in the training data, doesn't guarantee that it is impossible.\n",
    "\n",
    "There are many smoothing algorithms but the simplest one is called **Laplace smoothing** or **Add-one smoothing**. The part of the classification algorithm which causes a problem is the probability of a feature given a class *P(f<sub>i</sub>|c)*, which we interpret as the **fraction of times the word w<sub>i</sub> appears among all words in all documents of topic c**. If the numerator is equal to 0, the whole probability is also equal to 0. Laplace smoothing adds 1 to each count resulting in a new formula for the probability. Note that since we artificially increment the number of occurrences of each word in the numerator and denominator, we add the size of the vocabulary |V| to the denominator. This results in the equation:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq10.png\" alt=\"equation\" width=\"600\"/></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Completed NB example & Implementation\n",
    "\n",
    "Ok, so let's complete our example using the Laplace smoothing.\n",
    "\n",
    "The size of our vocabulary |V| = 7, so we will add it to all denominators.\n",
    "\n",
    "| word | P(word\\|sports) | P(word\\|not sports)|\n",
    "|------|----------------|-------------------|\n",
    "| close | 1/10 | 2/11 |\n",
    "| great | 2/10| 1/11|\n",
    "| match | 3/10| 1/11|\n",
    "\n",
    "P(\"close great match\"|sports) = 1/10 * 2/10 * 3/10 = 0.006\n",
    "P(\"close great match\"|not sports) = 2/11 * 1/11 * 1/11 = 0.0015\n",
    "\n",
    "Now, we have to multiply each probability by the probability of a class (posterior = likelihood * prior), which results in:\n",
    "P(\"close great match\"|sports)*P(sports) = 0.006 * 0.5 = 0.003\n",
    "P(\"close great match\"|not sports)*P(not sports) = 0.0015 * 0.5 = 0.00075\n",
    "\n",
    "Thus, there is a higher probability that the test document is indeed about sports and this would be the decision of the NB classifier.\n",
    "\n",
    "Now, let's implement the same example using `Python` and `scikit-learn`! Firstly let's create a training set and a test document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - sports, 0 - not sports\n",
    "training_corpus = [\"What a great match\",\n",
    "          \"The election results will be out tomorrow\",\n",
    "          \"The match was very boring\",\n",
    "          \"It was a close election\",\n",
    "          ]\n",
    "training_labels = [1, 0, 1, 0]\n",
    "\n",
    "test_doc = \"that was a very close, great match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to preprocess and vectorize documents to extract features (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boring', 'close', 'election', 'great', 'match', 'results', 'tomorrow']\n",
      "[[0 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 1 1]\n",
      " [1 0 0 0 1 0 0]\n",
      " [0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate countvectorizer. You can sepcify what kind of preprocessing will be done by the CountVectorizer\n",
    "# In this case we want all text in lowercase, remove stopwords, keep only alphanumeric characters.\n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "\n",
    "# Fit training data\n",
    "training_data = count_vector.fit_transform(training_corpus)\n",
    "# Let's inspect vectors\n",
    "print(count_vector.get_feature_names())\n",
    "print(training_data.toarray())\n",
    "\n",
    "# transform test data\n",
    "test_doc_transform = count_vector.transform([test_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After vectorizing, let's create and train the NB classifier. For numerical data, we have used a Gaussian NB classifier. For the NLP applications, we will use the Multinomial version of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the classifier object and fit data. The classifier object is by default created with the Laplace smoothing\n",
    "naive_bayes = MultinomialNB(alpha=1)  # alpha parameter specifies the k number from the add-k smoothing. 1 is the default value\n",
    "naive_bayes.fit(training_data, training_labels)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = naive_bayes.predict(test_doc_transform)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of 1, means the classifier predicts that the test sentence was from the \"sport\" category. Now, let's try with this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc2 = \"that was a very close, great election\"\n",
    "test_doc2_transform = count_vector.transform([test_doc2])\n",
    "predictions = naive_bayes.predict(test_doc2_transform)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[[0.20429777 0.79570223]]\n",
      "[[0.6979549 0.3020451]]\n"
     ]
    }
   ],
   "source": [
    "# We can also inspect calculated probabilites.\n",
    "print(naive_bayes.classes_)  # print the order of classes\n",
    "print(naive_bayes.predict_proba(test_doc_transform))\n",
    "print(naive_bayes.predict_proba(test_doc2_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just a simple example to familiarize with the NB classifier. Let's look at a real word scenario!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Logistic Regression Classifier\n",
    "\n",
    "### 9.1 LR Fundamentals\n",
    "\n",
    "\n",
    "Logistic Regression is the another supervised learning algorithm and probabilistic classifier. This means that it will be also interested in calculating the probability *P(c|d)*:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq1.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "NB classifier used the Bayes' theorem and additional assumptions to derive another equation from the one above. NB uses the probability of a document as a set of features given a class *P(d|c)*, which was learned from the training dataset. \n",
    "\n",
    "Logistic regression does it differently. As a discriminative classifier, it tries to learn differences between classes rather than how class representatives \"look like\". Thus, it will directly try to learn the probability of a class given a set of features representing a document *P(c|d)*. The key of this algorithm is in determining which features discriminate documents most efficiently, by assigning these features appropriate **weights** (parameters). For example, in the \"sports\" or \"not sports\" text classification, the word \"football\" will probably\n",
    "have strong positive weight and the word \"princess\", probably strong negative weight. \n",
    "\n",
    "In supervised machine learning, we have input features and sets of labels. To make predictions based on data, we use a function F with some parameters Θ to map features to output labels. To get an optimum mapping from features to labels, we have to minimize the cost function, which works by comparing how closely the output Ŷ is to the true labels Y from the training data. Learning takes place by updating the parameters and repeating the process until the cost is minimized.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/sup_learning.jpeg\" alt=\"equation\" width=\"600\"/></div>\n",
    "\n",
    "Parts of the Logistic Regression algorithm:\n",
    "1. Feature representation - in our case using one-hot vectors\n",
    "2. A classification function that gives the estimated class using *P(c|d)* - we will use two most popular ones - **sigmoid** (binomial LR) and **softmax** (multinomial LR)\n",
    "3. The cost function (or loss function) used for learning, which tells us the difference between the estimated class ĉ and the true class c. We will use **cross-entropy loss function**\n",
    "4. An algorithm for optimizing the classifier by adjusting parameters and consequently minimizing the cost function. One of them is the **Stochastic Gradient Decent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we represent features using a vector `x=[x`<sub>`1`</sub>`, x`<sub>`2`</sub>`, x`<sub>`3`</sub>` ... x`<sub>`n`</sub>`]` and we need to associate some weights with each of these features, we can also represent these weights using a **weights vector** `w=[w`<sub>`1`</sub>`, w`<sub>`2`</sub>`, w`<sub>`3`</sub>` ... w`<sub>`n`</sub>`]`. Then, we can calculate the score for the test document by multiplying each feature with the associated weight and adding all results together. There is also another parameter - the **bias term** `b`, which is added to the sum.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq11.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "In other words, we want the sum of the **dot product** of the features and weights vectors and the bias term:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq12.png\" alt=\"equation\" width=\"300\"/></div>\n",
    "\n",
    "But how do we map this score to the probability, since the score can be any real number from −∞ to ∞? We will use a **sigmoid function** *σ(z)*, which is also called the logistic function.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq13.png\" alt=\"equation\" width=\"700\"/></div>\n",
    "\n",
    "As you can see, it is symmetric with respect to the point (0, 0.5) and it maps all real numbers to the range (0,1) - this is exactly what we need! For two classes (binomial logistic regression), class 0 and class 1, we have probabilities as follows:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/TheRootOf3/ucl-nlp-notebook-series/main/Notebook4/nb_resources/eq14.png\" alt=\"equation\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "Okay, so how does the classifier learn the correct weights vector and the bias? It starts with some arbitrary values, which are then used to classify a training set. Each result is compared with the true label using the loss functions, in our case with the cross-entropy loss function. This function tells the classifier what is the difference between the classifier output and the true label. Taking into account loss, the classifier needs to adjust its parameters *θ* (in our case *θ = w, b*) to minimize the loss. This is equivalent to finding parameters *θ*, that minimize the loss function (find its minimum). \n",
    "\n",
    "#### Minimizing the loss\n",
    "\n",
    "Since the loss function is parameterized by weights and the bias term, finding its minimum is a multidimensional task. The popular algorithm for finding the minimum of a function is the Stochastic Gradient Decent (SGD). Gradient Descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters *θ*) the function’s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself 360 degrees, find the direction where the ground is sloping the steepest, and walk downhill in that direction.\n",
    "\n",
    "I strongly encourage you to read [this chapter](https://web.stanford.edu/~jurafsky/slp3/5.pdf) of the book “Speech and Language Processing” by Daniel Jurafsky and James H. Martin as it explains the idea behind SGD and Logistic Regression very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Having all this knowledge, let's try to implement a Logistic Regression classifier on the BBC dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 LR example using the bbc news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_data = pd.read_csv(bbc_dataset_file)\n",
    "bbc_data = bbc_data[['category', 'text']]\n",
    "bbc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize words, split dataset into training and testing and fit the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate countvectorizer \n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', token_pattern='\\w+')\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bbc_data['text'],\n",
    "    bbc_data['category'],\n",
    "    test_size=0.2,\n",
    "    random_state=50,\n",
    ")\n",
    "\n",
    "# remember to fit count vectorizer on the training data, because only these words are present in the training set. \n",
    "# Adding words from the testing set makes this set no longer \"unseen\" to the model\n",
    "X_train_trans = count_vector.fit_transform(X_train)\n",
    "y_train_trans = le.fit_transform(y_train)\n",
    "\n",
    "# Logistic Regression classifier has several parameters including the penalty for big weights (l1, l2) and the method of finding the loss function minimum (solver)\n",
    "lr = LogisticRegression(random_state=0, penalty='l2', solver='lbfgs', verbose=0)\n",
    "lr.fit(X_train_trans, y_train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9617977528089887"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_trans = count_vector.transform(X_test)\n",
    "y_test_trans = le.transform(y_test)\n",
    "\n",
    "predictions = lr.predict(X_test_trans)\n",
    "accuracy_score(y_test_trans, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a simple example of how the classifier performs on a tech article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_article = X_train[0]\n",
    "tech_article[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 2.2123677537131186e-08,\n",
       " 'entertainment': 0.0001419323144260903,\n",
       " 'politics': 6.029509338152133e-07,\n",
       " 'sport': 3.553227736301087e-09,\n",
       " 'tech': 0.9998574390577348}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(le.inverse_transform(lr.classes_), *lr.predict_proba(count_vector.transform([tech_article]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing we can do is to inspect which words have the most positive and most negative weights (are influencing the classification most). The `lr.coef_` gives us weights trained by the classifier for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.77008159e-02 -4.31583235e-03  4.22547508e-02 ... -5.44355983e-04\n",
      "  -2.10169487e-08 -2.09024382e-04]\n",
      " [-6.02582168e-02  6.18273670e-03  6.68598754e-02 ...  6.64317897e-03\n",
      "  -1.03485583e-05  2.50823664e-03]\n",
      " [-2.50935813e-02 -6.19608969e-05  2.08950764e-02 ... -8.69366923e-04\n",
      "  -5.50843282e-07 -4.68280347e-05]\n",
      " [ 1.05872546e-01 -5.34020719e-05 -1.16065069e-01 ... -4.88809180e-03\n",
      "   1.25195113e-05 -1.61260505e-03]\n",
      " [-3.82215641e-02 -1.75154138e-03 -1.39446338e-02 ... -3.41364259e-04\n",
      "  -1.59909281e-06 -6.39779168e-04]]\n",
      "Shape: (5, 25597)\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_)\n",
    "print(\"Shape: {}\".format(np.shape(lr.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each class has separate weights (there are 5 classes and 5 dimensions), as this is a Multinomial Logistic Regression classification task. Let's see the top-weighted words for each class. Firstly, we need to associate classes names with values, then we can see which words discriminate classes in the best way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business' 'entertainment' 'politics' 'sport' 'tech']\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Classes names are mapped to numbers (we used the label encoder for this, so let's see what is the order)\n",
    "class_names = le.inverse_transform(lr.classes_)\n",
    "print(class_names)\n",
    "print(lr.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = {}\n",
    "weights_dict[\"feature\"] = count_vector.get_feature_names()\n",
    "for class_num in range(5):\n",
    "    weights_dict[class_names[class_num]] = lr.coef_[class_num]\n",
    "\n",
    "bbc_features_weights = pd.DataFrame(weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>firm</td>\n",
       "      <td>0.571839</td>\n",
       "      <td>-0.288587</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>-0.181756</td>\n",
       "      <td>-0.019078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>euros</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>-0.152402</td>\n",
       "      <td>-0.064496</td>\n",
       "      <td>-0.149050</td>\n",
       "      <td>-0.099961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20804</th>\n",
       "      <td>shares</td>\n",
       "      <td>0.384076</td>\n",
       "      <td>-0.132941</td>\n",
       "      <td>-0.037758</td>\n",
       "      <td>-0.124302</td>\n",
       "      <td>-0.089076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>bank</td>\n",
       "      <td>0.360531</td>\n",
       "      <td>-0.113132</td>\n",
       "      <td>-0.065330</td>\n",
       "      <td>-0.109061</td>\n",
       "      <td>-0.073009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18146</th>\n",
       "      <td>profits</td>\n",
       "      <td>0.340768</td>\n",
       "      <td>-0.084489</td>\n",
       "      <td>-0.039651</td>\n",
       "      <td>-0.103981</td>\n",
       "      <td>-0.112647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>britain</td>\n",
       "      <td>-0.201828</td>\n",
       "      <td>-0.062542</td>\n",
       "      <td>0.246412</td>\n",
       "      <td>0.072051</td>\n",
       "      <td>-0.054093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25144</th>\n",
       "      <td>win</td>\n",
       "      <td>-0.204785</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>-0.033466</td>\n",
       "      <td>0.399809</td>\n",
       "      <td>-0.089665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16365</th>\n",
       "      <td>old</td>\n",
       "      <td>-0.213929</td>\n",
       "      <td>0.015799</td>\n",
       "      <td>-0.030318</td>\n",
       "      <td>0.211419</td>\n",
       "      <td>0.017030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>film</td>\n",
       "      <td>-0.249245</td>\n",
       "      <td>0.567778</td>\n",
       "      <td>-0.145280</td>\n",
       "      <td>-0.196645</td>\n",
       "      <td>0.023392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>163</td>\n",
       "      <td>-0.448092</td>\n",
       "      <td>0.645419</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>-0.100412</td>\n",
       "      <td>-0.069067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature  business  entertainment  politics     sport      tech\n",
       "9518      firm  0.571839      -0.288587 -0.082418 -0.181756 -0.019078\n",
       "8757     euros  0.465909      -0.152402 -0.064496 -0.149050 -0.099961\n",
       "20804   shares  0.384076      -0.132941 -0.037758 -0.124302 -0.089076\n",
       "2955      bank  0.360531      -0.113132 -0.065330 -0.109061 -0.073009\n",
       "18146  profits  0.340768      -0.084489 -0.039651 -0.103981 -0.112647\n",
       "...        ...       ...            ...       ...       ...       ...\n",
       "4001   britain -0.201828      -0.062542  0.246412  0.072051 -0.054093\n",
       "25144      win -0.204785      -0.071892 -0.033466  0.399809 -0.089665\n",
       "16365      old -0.213929       0.015799 -0.030318  0.211419  0.017030\n",
       "9433      film -0.249245       0.567778 -0.145280 -0.196645  0.023392\n",
       "225        163 -0.448092       0.645419 -0.027848 -0.100412 -0.069067\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"business\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>163</td>\n",
       "      <td>-0.448092</td>\n",
       "      <td>0.645419</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>-0.100412</td>\n",
       "      <td>-0.069067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>film</td>\n",
       "      <td>-0.249245</td>\n",
       "      <td>0.567778</td>\n",
       "      <td>-0.145280</td>\n",
       "      <td>-0.196645</td>\n",
       "      <td>0.023392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21128</th>\n",
       "      <td>singer</td>\n",
       "      <td>-0.121295</td>\n",
       "      <td>0.393541</td>\n",
       "      <td>-0.076322</td>\n",
       "      <td>-0.119649</td>\n",
       "      <td>-0.076276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23798</th>\n",
       "      <td>tv</td>\n",
       "      <td>-0.124427</td>\n",
       "      <td>0.353531</td>\n",
       "      <td>-0.144471</td>\n",
       "      <td>-0.165795</td>\n",
       "      <td>0.081160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15650</th>\n",
       "      <td>music</td>\n",
       "      <td>-0.201312</td>\n",
       "      <td>0.352136</td>\n",
       "      <td>-0.126266</td>\n",
       "      <td>-0.192007</td>\n",
       "      <td>0.167450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22913</th>\n",
       "      <td>technology</td>\n",
       "      <td>-0.201781</td>\n",
       "      <td>-0.176289</td>\n",
       "      <td>-0.056054</td>\n",
       "      <td>-0.117837</td>\n",
       "      <td>0.551962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10150</th>\n",
       "      <td>games</td>\n",
       "      <td>-0.088289</td>\n",
       "      <td>-0.196881</td>\n",
       "      <td>-0.022945</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.297390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10139</th>\n",
       "      <td>game</td>\n",
       "      <td>-0.188651</td>\n",
       "      <td>-0.220367</td>\n",
       "      <td>-0.061090</td>\n",
       "      <td>0.196689</td>\n",
       "      <td>0.273420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10599</th>\n",
       "      <td>government</td>\n",
       "      <td>0.195998</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.349831</td>\n",
       "      <td>-0.150857</td>\n",
       "      <td>-0.150576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>firm</td>\n",
       "      <td>0.571839</td>\n",
       "      <td>-0.288587</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>-0.181756</td>\n",
       "      <td>-0.019078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  business  entertainment  politics     sport      tech\n",
       "225           163 -0.448092       0.645419 -0.027848 -0.100412 -0.069067\n",
       "9433         film -0.249245       0.567778 -0.145280 -0.196645  0.023392\n",
       "21128      singer -0.121295       0.393541 -0.076322 -0.119649 -0.076276\n",
       "23798          tv -0.124427       0.353531 -0.144471 -0.165795  0.081160\n",
       "15650       music -0.201312       0.352136 -0.126266 -0.192007  0.167450\n",
       "...           ...       ...            ...       ...       ...       ...\n",
       "22913  technology -0.201781      -0.176289 -0.056054 -0.117837  0.551962\n",
       "10150       games -0.088289      -0.196881 -0.022945  0.010725  0.297390\n",
       "10139        game -0.188651      -0.220367 -0.061090  0.196689  0.273420\n",
       "10599  government  0.195998      -0.244397  0.349831 -0.150857 -0.150576\n",
       "9518         firm  0.571839      -0.288587 -0.082418 -0.181756 -0.019078\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"entertainment\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16982</th>\n",
       "      <td>party</td>\n",
       "      <td>-0.173311</td>\n",
       "      <td>-0.139301</td>\n",
       "      <td>0.478468</td>\n",
       "      <td>-0.099592</td>\n",
       "      <td>-0.066263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>blair</td>\n",
       "      <td>-0.163510</td>\n",
       "      <td>-0.110918</td>\n",
       "      <td>0.384272</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>-0.064248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10599</th>\n",
       "      <td>government</td>\n",
       "      <td>0.195998</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.349831</td>\n",
       "      <td>-0.150857</td>\n",
       "      <td>-0.150576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13475</th>\n",
       "      <td>labour</td>\n",
       "      <td>-0.091466</td>\n",
       "      <td>-0.108259</td>\n",
       "      <td>0.331888</td>\n",
       "      <td>-0.083972</td>\n",
       "      <td>-0.048192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8242</th>\n",
       "      <td>election</td>\n",
       "      <td>-0.084945</td>\n",
       "      <td>-0.121483</td>\n",
       "      <td>0.325777</td>\n",
       "      <td>-0.067464</td>\n",
       "      <td>-0.051885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>balls</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.125880</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15650</th>\n",
       "      <td>music</td>\n",
       "      <td>-0.201312</td>\n",
       "      <td>0.352136</td>\n",
       "      <td>-0.126266</td>\n",
       "      <td>-0.192007</td>\n",
       "      <td>0.167450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>companies</td>\n",
       "      <td>0.215547</td>\n",
       "      <td>-0.036713</td>\n",
       "      <td>-0.133618</td>\n",
       "      <td>-0.086322</td>\n",
       "      <td>0.041106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23798</th>\n",
       "      <td>tv</td>\n",
       "      <td>-0.124427</td>\n",
       "      <td>0.353531</td>\n",
       "      <td>-0.144471</td>\n",
       "      <td>-0.165795</td>\n",
       "      <td>0.081160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>film</td>\n",
       "      <td>-0.249245</td>\n",
       "      <td>0.567778</td>\n",
       "      <td>-0.145280</td>\n",
       "      <td>-0.196645</td>\n",
       "      <td>0.023392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  business  entertainment  politics     sport      tech\n",
       "16982       party -0.173311      -0.139301  0.478468 -0.099592 -0.066263\n",
       "3495        blair -0.163510      -0.110918  0.384272 -0.045596 -0.064248\n",
       "10599  government  0.195998      -0.244397  0.349831 -0.150857 -0.150576\n",
       "13475      labour -0.091466      -0.108259  0.331888 -0.083972 -0.048192\n",
       "8242     election -0.084945      -0.121483  0.325777 -0.067464 -0.051885\n",
       "...           ...       ...            ...       ...       ...       ...\n",
       "2921        balls  0.127090      -0.000990 -0.125880 -0.000093 -0.000127\n",
       "15650       music -0.201312       0.352136 -0.126266 -0.192007  0.167450\n",
       "5560    companies  0.215547      -0.036713 -0.133618 -0.086322  0.041106\n",
       "23798          tv -0.124427       0.353531 -0.144471 -0.165795  0.081160\n",
       "9433         film -0.249245       0.567778 -0.145280 -0.196645  0.023392\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"politics\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25144</th>\n",
       "      <td>win</td>\n",
       "      <td>-0.204785</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>-0.033466</td>\n",
       "      <td>0.399809</td>\n",
       "      <td>-0.089665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14700</th>\n",
       "      <td>match</td>\n",
       "      <td>-0.166496</td>\n",
       "      <td>-0.127954</td>\n",
       "      <td>-0.040573</td>\n",
       "      <td>0.396486</td>\n",
       "      <td>-0.061463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22890</th>\n",
       "      <td>team</td>\n",
       "      <td>-0.107919</td>\n",
       "      <td>-0.126025</td>\n",
       "      <td>-0.096541</td>\n",
       "      <td>0.361067</td>\n",
       "      <td>-0.030583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>cup</td>\n",
       "      <td>-0.137345</td>\n",
       "      <td>-0.090435</td>\n",
       "      <td>-0.037199</td>\n",
       "      <td>0.336211</td>\n",
       "      <td>-0.071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12311</th>\n",
       "      <td>injury</td>\n",
       "      <td>-0.122379</td>\n",
       "      <td>-0.078572</td>\n",
       "      <td>-0.049308</td>\n",
       "      <td>0.290245</td>\n",
       "      <td>-0.039987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23860</th>\n",
       "      <td>uk</td>\n",
       "      <td>-0.102038</td>\n",
       "      <td>0.116291</td>\n",
       "      <td>0.206630</td>\n",
       "      <td>-0.170610</td>\n",
       "      <td>-0.050274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>firm</td>\n",
       "      <td>0.571839</td>\n",
       "      <td>-0.288587</td>\n",
       "      <td>-0.082418</td>\n",
       "      <td>-0.181756</td>\n",
       "      <td>-0.019078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15650</th>\n",
       "      <td>music</td>\n",
       "      <td>-0.201312</td>\n",
       "      <td>0.352136</td>\n",
       "      <td>-0.126266</td>\n",
       "      <td>-0.192007</td>\n",
       "      <td>0.167450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>film</td>\n",
       "      <td>-0.249245</td>\n",
       "      <td>0.567778</td>\n",
       "      <td>-0.145280</td>\n",
       "      <td>-0.196645</td>\n",
       "      <td>0.023392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>mr</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.019606</td>\n",
       "      <td>0.289936</td>\n",
       "      <td>-0.458096</td>\n",
       "      <td>0.051266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  business  entertainment  politics     sport      tech\n",
       "25144     win -0.204785      -0.071892 -0.033466  0.399809 -0.089665\n",
       "14700   match -0.166496      -0.127954 -0.040573  0.396486 -0.061463\n",
       "22890    team -0.107919      -0.126025 -0.096541  0.361067 -0.030583\n",
       "6495      cup -0.137345      -0.090435 -0.037199  0.336211 -0.071232\n",
       "12311  injury -0.122379      -0.078572 -0.049308  0.290245 -0.039987\n",
       "...       ...       ...            ...       ...       ...       ...\n",
       "23860      uk -0.102038       0.116291  0.206630 -0.170610 -0.050274\n",
       "9518     firm  0.571839      -0.288587 -0.082418 -0.181756 -0.019078\n",
       "15650   music -0.201312       0.352136 -0.126266 -0.192007  0.167450\n",
       "9433     film -0.249245       0.567778 -0.145280 -0.196645  0.023392\n",
       "15559      mr  0.097288       0.019606  0.289936 -0.458096  0.051266\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"sport\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22913</th>\n",
       "      <td>technology</td>\n",
       "      <td>-0.201781</td>\n",
       "      <td>-0.176289</td>\n",
       "      <td>-0.056054</td>\n",
       "      <td>-0.117837</td>\n",
       "      <td>0.551962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15320</th>\n",
       "      <td>mobile</td>\n",
       "      <td>-0.105172</td>\n",
       "      <td>-0.120836</td>\n",
       "      <td>-0.062559</td>\n",
       "      <td>-0.097270</td>\n",
       "      <td>0.385838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>233</td>\n",
       "      <td>-0.191574</td>\n",
       "      <td>-0.090254</td>\n",
       "      <td>-0.025219</td>\n",
       "      <td>-0.067974</td>\n",
       "      <td>0.375021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21441</th>\n",
       "      <td>software</td>\n",
       "      <td>-0.153330</td>\n",
       "      <td>-0.084149</td>\n",
       "      <td>-0.084437</td>\n",
       "      <td>-0.044048</td>\n",
       "      <td>0.365964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16410</th>\n",
       "      <td>online</td>\n",
       "      <td>-0.059032</td>\n",
       "      <td>-0.118098</td>\n",
       "      <td>-0.084213</td>\n",
       "      <td>-0.079578</td>\n",
       "      <td>0.340920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16490</th>\n",
       "      <td>orange</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>-0.016963</td>\n",
       "      <td>-0.015476</td>\n",
       "      <td>-0.019532</td>\n",
       "      <td>-0.114828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20102</th>\n",
       "      <td>said</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>0.040237</td>\n",
       "      <td>0.150887</td>\n",
       "      <td>-0.070312</td>\n",
       "      <td>-0.117303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10599</th>\n",
       "      <td>government</td>\n",
       "      <td>0.195998</td>\n",
       "      <td>-0.244397</td>\n",
       "      <td>0.349831</td>\n",
       "      <td>-0.150857</td>\n",
       "      <td>-0.150576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22695</th>\n",
       "      <td>t</td>\n",
       "      <td>-0.107107</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.042036</td>\n",
       "      <td>0.187509</td>\n",
       "      <td>-0.159156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>s</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.129987</td>\n",
       "      <td>-0.049067</td>\n",
       "      <td>0.096388</td>\n",
       "      <td>-0.182781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25597 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  business  entertainment  politics     sport      tech\n",
       "22913  technology -0.201781      -0.176289 -0.056054 -0.117837  0.551962\n",
       "15320      mobile -0.105172      -0.120836 -0.062559 -0.097270  0.385838\n",
       "500           233 -0.191574      -0.090254 -0.025219 -0.067974  0.375021\n",
       "21441    software -0.153330      -0.084149 -0.084437 -0.044048  0.365964\n",
       "16410      online -0.059032      -0.118098 -0.084213 -0.079578  0.340920\n",
       "...           ...       ...            ...       ...       ...       ...\n",
       "16490      orange  0.166800      -0.016963 -0.015476 -0.019532 -0.114828\n",
       "20102        said -0.003508       0.040237  0.150887 -0.070312 -0.117303\n",
       "10599  government  0.195998      -0.244397  0.349831 -0.150857 -0.150576\n",
       "22695           t -0.107107       0.036718  0.042036  0.187509 -0.159156\n",
       "20051           s  0.005473       0.129987 -0.049067  0.096388 -0.182781\n",
       "\n",
       "[25597 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_features_weights.sort_values(by=\"tech\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3afa0fa510e8fcdc377ad5072d6b62de959f50d8f4193a0bfb045b2c849d2b0a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
